from openai import OpenAI
import re
import streamlit as st
from .prompt_config import ACADEMIC_PROMPTS, SYSTEM_PROMPTS
from .model_config import MODEL_PROVIDERS

# è¾…åŠ©å‡½æ•°--æå–è‹±æ–‡å…³é”®è¯
# def extract_english_keywords(text: str) -> list:
#     match = re.search(r'è‹±æ–‡å…³é”®è¯:\s*([^\n;]+)', text)
#     if not match:
#         return []
#     keywords_str = match.group(1).strip().rstrip(';')
#     return [kw.strip() for kw in keywords_str.split(',')]

import re


def extract_english_keywords(text: str) -> list:
    """
    æ›´å¥å£®çš„è‹±æ–‡å…³é”®è¯æå–å‡½æ•°

    ç‰¹ç‚¹ï¼š
    1. ä¸ä¾èµ–ä¸¥æ ¼çš„æ ‡ç‚¹ç¬¦å·æ ¼å¼
    2. æ”¯æŒå¤šç§åˆ†éš”ç¬¦ï¼ˆé€—å·ã€åˆ†å·ã€ç©ºæ ¼ç­‰ï¼‰
    3. è‡ªåŠ¨è¿‡æ»¤ç©ºç™½å’Œæ— æ•ˆé¡¹

    ç¤ºä¾‹è¾“å…¥ï¼š
    "è‹±æ–‡å…³é”®è¯: llm, hfl, md" -> ["llm", "hfl", "md"]
    "è‹±æ–‡å…³é”®è¯ llm hfl md" -> ["llm", "hfl", "md"]
    "è‹±æ–‡å…³é”®è¯ï¼šllmï¼›hflï¼›md" -> ["llm", "hfl", "md"]
    """
    # æ›´çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é…å¤šç§æ ¼å¼
    match = re.search(
        r'(?:è‹±æ–‡å…³é”®è¯|keywords?)[:\s]*([^;\n]+)',
        text,
        re.IGNORECASE  # ä¸åŒºåˆ†å¤§å°å†™
    )

    if not match:
        return []

    # æå–å…³é”®è¯éƒ¨åˆ†å¹¶å¤„ç†
    keywords_str = match.group(1).strip()

    # æ”¯æŒå¤šç§åˆ†éš”ç¬¦ï¼šé€—å·ã€åˆ†å·ã€ç©ºæ ¼ç­‰
    keywords = re.split(r'[,;\s]\s*', keywords_str)

    # è¿‡æ»¤å¤„ç†
    return [kw.strip() for kw in keywords if kw.strip()]

# è¾…åŠ©å‡½æ•°--æå–åˆ†æ•°
# def extract_score(text: str) -> int:
#
#     parts = text.split(":")
#     if len(parts) == 2:
#         return int(parts[1].strip())
#     else:
#         raise ValueError("æ–‡æœ¬æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")

def extract_score(text: str) -> dict:
    """
    ä»LLMå“åº”ä¸­æå–è¯„åˆ†ä¿¡æ¯çš„é²æ£’æ–¹æ³•

    è¿”å›: {
        "total": int,
        "keyword": int,
        "semantic": int
    }
    """
    patterns = {
        "total": r"æ€»è¯„åˆ†:\s*(\d+)",
        "keyword": r"å…³é”®è¯å¾—åˆ†:\s*(\d+)",
        "semantic": r"è¯­ä¹‰å¾—åˆ†:\s*(\d+)"
    }

    scores = {}
    for key, pattern in patterns.items():
        match = re.search(pattern, text)
        if match:
            try:
                scores[key] = int(match.group(1))
            except (ValueError, IndexError):
                scores[key] = 0

    # ç¡®ä¿è‡³å°‘è¿”å›æ€»åˆ†
    if "total" not in scores:
        raise ValueError("æ— æ³•ä»æ–‡æœ¬ä¸­æå–è¯„åˆ†ä¿¡æ¯: \n" + text)

    return scores

# å®šä¹‰è®¿é—®url
def get_openai_client():
    if not st.session_state.api_key:
        st.error("è¯·è¾“å…¥API Key")
        return None

    return OpenAI(
        api_key=st.session_state.api_key,
        base_url=MODEL_PROVIDERS[st.session_state.model_provider]["base_url"]
    )

# æå–å…³é”®è¯çš„llmè®¾ç½®
def get_keywords_from_query(client, user_query: str, data_source) -> list:
    completion = client.chat.completions.create(
        model=st.session_state.keyword_model,
        messages=[
            {'role': 'system', 'content': SYSTEM_PROMPTS["keyword_expert"].format(paper_source=data_source)},
            {'role': 'user', 'content': user_query}
        ]
    )
    return extract_english_keywords(completion.choices[0].message.content)

# è·å–referenceçš„llmè®¾ç½®
def get_reference(client, english_summary: str) -> str:
    completion = client.chat.completions.create(
        model=st.session_state.similarity_model,
        messages=[
            {'role': 'system', 'content': SYSTEM_PROMPTS["reference_expert"]},
            {'role': 'user', 'content': english_summary}
        ],
        temperature=0  # å®Œå…¨ç¡®å®šæ€§è¾“å‡º
    )
    return completion.choices[0].message.content


# è·å–ä¸­æ–‡æ‘˜è¦çš„llmè®¾ç½®
def get_chinese_summary(client, english_summary: str) -> str:
    completion = client.chat.completions.create(
        model=st.session_state.similarity_model,
        messages=[
            {'role': 'system', 'content': SYSTEM_PROMPTS["translation_expert"]},
            {'role': 'user', 'content': english_summary}
        ]
    )
    return completion.choices[0].message.content

# å¯¹æ‰€æœ‰æ–‡ç« è®¡ç®—è¯„åˆ†å¹¶è¿”å›ç»“æœ
def sort_score(client, results, query) -> list:
    scored_articles = []
    for article in results:
        # ç»Ÿä¸€è·å–æ‘˜è¦ä»¥åŠæ ‡é¢˜--Arxivä¸IEEE
        abstract = getattr(article, 'summary', getattr(article, 'Abstract', article.get('abstract') if isinstance(article, dict) else None))
        # title = article.title if hasattr(article, 'title') else article['title']
        completion = client.chat.completions.create(
            model=st.session_state.similarity_model,
            messages=[
                {'role': 'system', 'content': SYSTEM_PROMPTS["similarity_expert"]},
                {'role': 'user', 'content': f"ç”¨æˆ·éœ€æ±‚: {query}\næ–‡ç« æ‘˜è¦: {abstract}"}
            ]
        )
        try:
            score_dict  = extract_score(completion.choices[0].message.content)
            scored_articles.append((score_dict["total"], article))
        except Exception as e:
            st.error(f"è¯„åˆ†å‡ºé”™: {str(e)}")
            scored_articles.append((0, article))
    return scored_articles

st.markdown(
    """
        <style>
            /* å›ºå®šå®¹å™¨é«˜åº¦å’Œæ»šåŠ¨æ ·å¼ */
            .fixed-content {
                height: 10px;  /* è®¾ç½®å›ºå®šé«˜åº¦ */
                overflow-y: auto;  /* æ·»åŠ å‚ç›´æ»šåŠ¨æ¡ */
                padding: 1rem;
                border: 1px solid #ddd;
                border-radius: 4px;
            }
    
            /* ç¾åŒ–æ»šåŠ¨æ¡ */
            .fixed-content::-webkit-scrollbar {
                width: 12px;
                background-color: #F5F5F5;
            }
    
            .fixed-content::-webkit-scrollbar-thumb {
                border-radius: 10px;
                -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,.3);
                background-color: #555;
            }
    
            /* PDFå®¹å™¨æ ·å¼ä¿æŒä¸å˜ */
            .pdf-container {
                height: 800px;
                overflow-y: scroll;
                border: 1px solid #ddd;
                border-radius: 4px;
                padding: 10px;
            }
    
            .pdf-container iframe {
                width: 100%;
                height: 100%;
                border: none;
            }
    
            /* ç¡®ä¿åˆ—å†…å®¹ä¸ä¼šè¶…å‡º */
            .stColumn {
                height: 800px;
                overflow-y: auto;
            }
        </style>
    """, unsafe_allow_html=True)

# æ–‡çŒ®åˆ†æ---æµå¼è¾“å‡º
def get_streaming_response(client, messages):
    """è·å–æµå¼å“åº”"""
    try:
        # åˆ›å»ºæµå¼å“åº”
        stream = client.chat.completions.create(
            model=st.session_state.similarity_model,
            messages=messages,
            stream=True  # å¯ç”¨æµå¼è¾“å‡º
        )
        return stream
    except Exception as e:
        st.error(f"è·å–å“åº”æ—¶å‡ºé”™: {str(e)}")
        return None

# æ˜¾ç¤ºæµå¼å“åº”
def display_streaming_response(stream, placeholder):
    full_response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            full_response += chunk.choices[0].delta.content
            # å®æ—¶æ›´æ–°æ˜¾ç¤ºçš„å†…å®¹
            placeholder.markdown(full_response + "â–Œ")
    # å®Œæˆåç§»é™¤å…‰æ ‡
    placeholder.markdown(full_response)
    return full_response

# å¯¹è¯æ¨¡ç‰ˆè®¾ç½®
def render_chat_area(middle_col, client, operation_type):
    with middle_col:
        st.markdown('<div class="fixed-content">', unsafe_allow_html=True)
        st.markdown("## ğŸ’¬ å¯¹è¯åŒºåŸŸ")

        # åŸå­åŒ–çŠ¶æ€ç®¡ç†ï¼ˆç¡®ä¿å³æ—¶æ›´æ–°ï¼‰
        current_prompt = f"{ACADEMIC_PROMPTS[operation_type]}\n\nPDFå†…å®¹:\n{st.session_state.get('pdf_content', '')}"
        
        # ä½¿ç”¨æ·±å±‚çŠ¶æ€å¯¹æ¯”
        if "messages" not in st.session_state or not st.session_state.messages or \
           st.session_state.messages[0]["content"] != current_prompt:
            
            # ä¿ç•™éç³»ç»Ÿæ¶ˆæ¯çš„å†å²
            preserved_messages = [msg for msg in st.session_state.get("messages", []) 
                                 if msg["role"] != "system"]
            
            # åŸå­åŒ–æ›´æ–°ç³»ç»Ÿæç¤º
            st.session_state.messages = [
                {"role": "system", "content": current_prompt},
                *preserved_messages
            ]
            
            # å¼ºåˆ¶åŒæ­¥æ¸²æŸ“
            st.rerun()

        # æ˜¾ç¤ºå³æ—¶æ›´æ–°çš„å¯¹è¯
        for message in st.session_state.messages[1:]:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # å³æ—¶å“åº”ç”¨æˆ·è¾“å…¥
        if user_input := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
            # ä½¿ç”¨æœ€æ–°ä¸Šä¸‹æ–‡ç”Ÿæˆå“åº”
            with st.chat_message("user"):
                st.markdown(user_input)
            
            # æµå¼å“åº”å¤„ç†
            with st.chat_message("assistant"):
                full_response = ""
                placeholder = st.empty()
                
                # è·å–æœ€æ–°æ¶ˆæ¯ä¸Šä¸‹æ–‡
                messages = st.session_state.messages + [
                    {"role": "user", "content": user_input}
                ]
                
                stream = client.chat.completions.create(
                    model=st.session_state.similarity_model,
                    messages=messages,
                    stream=True
                )
                
                for chunk in stream:
                    text = chunk.choices[0].delta.content or ""
                    full_response += text
                    placeholder.markdown(full_response + "â–Œ")
                
                placeholder.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})

        st.markdown('</div>', unsafe_allow_html=True)
